# ğŸ§  EEG-to-Robot Control System

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Status: Active](https://img.shields.io/badge/Status-Active-green.svg)]()

A complete brain-computer interface system that translates EEG signals from Emotiv headsets into real-time robot arm control commands. This project combines advanced signal processing, machine learning, and robotics to create a direct neural control interface.

## ğŸ¯ **Project Overview**

This system processes neural signals in real-time to control a robot arm through thought patterns. The pipeline extracts frequency domain features from EEG data, processes them through a sophisticated neural network (CNN + Graph Neural Network + Transformer), and generates precise robot movement commands.

### **Key Features**
- ğŸ§  **Real-time EEG processing** from Emotiv headsets
- ğŸ¤– **Direct robot control** via neural signals
- ğŸ”¬ **Advanced ML architecture** (CNN + GCN + Transformer)
- ğŸ“Š **Frequency domain analysis** with standard neurological bands
- âš¡ **Low-latency processing** for responsive control
- ğŸ›ï¸ **Configurable parameters** for different use cases
- ğŸ“ˆ **Training pipeline** for custom models

### **Supported Hardware**
- **EEG Headsets**: Emotiv EPOC X, FLEX, Insight
- **Robots**: Universal Robots (UR3, UR5, UR10) via simulation
- **Platforms**: Linux, Windows, macOS

## ğŸš€ **Quick Start**

### **1. Installation**

#### **Automated Setup (Recommended)**
```bash
# Clone the repository
git clone https://github.com/MonashDeepNeuron/Motorola-Dream-Machine.git
cd Motorola-Dream-Machine

# Run automated setup
python scripts/setup.py

# Quick test
python scripts/quick_start.py --demo
```

#### **Manual Installation**
```bash
# Install core dependencies
pip install numpy scipy pyyaml pandas matplotlib scikit-learn

# Install optional dependencies (recommended)
pip install torch mne joblib seaborn plotly

# Install Emotiv SDK (for real headset)
pip install git+https://github.com/Emotiv/cortex-python.git
```

### **2. Quick Start**

#### **Demo Mode (No Hardware Required)**
```bash
# Run 60-second demo with mock data
python scripts/quick_start.py --demo
```

#### **Real-time System**
```bash
# Start full real-time system
python scripts/quick_start.py

# Run for specific duration
python scripts/quick_start.py --duration 300  # 5 minutes

# Run with custom config
python scripts/quick_start.py --config-dir my_config
```

#### **Individual Components**
```bash
# Test EEG processing
python src/eeg/processor.py --duration 10

# Test robot control
python src/robot/controller.py --interactive

# Test model inference
python src/model/inference.py --iterations 100

# Test Emotiv streaming
python src/eeg/emotiv_streamer.py --duration 30
```

### **3. Quick Demo**

```bash
# Start with a simple demonstration
./scripts/quick_start.sh
```

## ğŸ“‹ **System Requirements**

### **Hardware**
- **Computer**: 8GB+ RAM, multi-core CPU
- **GPU**: CUDA-compatible GPU recommended for training
- **EEG Headset**: Emotiv device with SDK access
- **Robot**: UR simulator or physical robot

### **Software**
- **Python**: 3.8 or higher
- **Operating System**: Linux (recommended), Windows, macOS
- **Dependencies**: See `requirements.txt`

## ğŸ—ï¸ **Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Emotiv    â”‚    â”‚     EEG      â”‚    â”‚   Machine   â”‚    â”‚    Robot     â”‚
â”‚   Headset   â”‚â”€â”€â”€â–¶â”‚  Processing  â”‚â”€â”€â”€â–¶â”‚  Learning   â”‚â”€â”€â”€â–¶â”‚   Control    â”‚
â”‚             â”‚    â”‚              â”‚    â”‚    Model    â”‚    â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                     â”‚                   â”‚                   â”‚
      â”‚              â€¢ FFT Analysis      â€¢ CNN Feature      â€¢ 3-axis movement
      â”‚              â€¢ Band Power        â€¢ Graph Conv       â€¢ Real-time commands
      â”‚              â€¢ Preprocessing     â€¢ Transformer      â€¢ JSON output
      â”‚                                  â€¢ Classification
```

### **Pipeline Components**

1. **EEG Signal Acquisition**: Real-time streaming from Emotiv headset
2. **Signal Processing**: Frequency domain analysis and feature extraction
3. **Machine Learning**: Advanced neural network for pattern recognition
4. **Robot Control**: Command generation and robot interface

## ğŸ“ **Project Structure**

```
Motorola-Dream-Machine/
â”œâ”€â”€ ğŸ“ src/                          # Main source code
â”‚   â”œâ”€â”€ ğŸ§  eeg/                      # EEG processing modules
â”‚   â”‚   â”œâ”€â”€ emotiv_streamer.py       # Real-time Emotiv interface
â”‚   â”‚   â”œâ”€â”€ processor.py             # Signal processing
â”‚   â”‚   â””â”€â”€ features.py              # Feature extraction
â”‚   â”œâ”€â”€ ğŸ¤– model/                    # Machine learning model
â”‚   â”‚   â”œâ”€â”€ eeg_model.py             # Neural network architecture
â”‚   â”‚   â””â”€â”€ trainer.py               # Training pipeline
â”‚   â”œâ”€â”€ ğŸ›ï¸ robot/                    # Robot control
â”‚   â”‚   â””â”€â”€ controller.py            # Robot command interface
â”‚   â”œâ”€â”€ ğŸ”§ utils/                    # Utilities
â”‚   â”‚   â”œâ”€â”€ config.py                # Configuration management
â”‚   â”‚   â””â”€â”€ logging.py               # Logging setup
â”‚   â”œâ”€â”€ demo.py                      # System demonstration
â”‚   â””â”€â”€ main.py                      # Main pipeline script
â”œâ”€â”€ ğŸ“ data/                         # Data storage
â”‚   â”œâ”€â”€ eeg_files/                   # Raw EEG recordings
â”‚   â”œâ”€â”€ processed/                   # Processed training data
â”‚   â””â”€â”€ models/                      # Trained model weights
â”œâ”€â”€ ğŸ“ config/                       # Configuration files
â”‚   â”œâ”€â”€ pipeline.yaml                # Main configuration
â”‚   â”œâ”€â”€ emotiv.yaml                  # Emotiv-specific settings
â”‚   â””â”€â”€ robot.yaml                   # Robot parameters
â”œâ”€â”€ ğŸ“ scripts/                      # Utility scripts
â”‚   â”œâ”€â”€ setup.sh                     # Environment setup
â”‚   â”œâ”€â”€ train.py                     # Model training
â”‚   â”œâ”€â”€ test_system.py               # System tests
â”‚   â””â”€â”€ quick_start.sh               # Quick start guide
â”œâ”€â”€ ğŸ“ notebooks/                    # Jupyter notebooks
â”‚   â”œâ”€â”€ data_exploration.ipynb       # Data analysis
â”‚   â””â”€â”€ model_analysis.ipynb         # Model visualization
â”œâ”€â”€ ğŸ“ docs/                         # Documentation
â”‚   â”œâ”€â”€ API.md                       # API documentation
â”‚   â”œâ”€â”€ TRAINING.md                  # Training guide
â”‚   â””â”€â”€ TROUBLESHOOTING.md           # Common issues
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ setup.py                         # Package setup
â””â”€â”€ README.md                        # This file
```

## ğŸ“ **Training Your Own Model**

### **1. Data Collection**

```bash
# Collect EEG data with labels
python src/main.py --mode collect --duration 300 --task motor_imagery

# Or use existing EDF files
python scripts/prepare_data.py --edf-files data/eeg_files/*.edf
```

### **2. Data Preprocessing**

```bash
# Process raw EEG into training features
python scripts/prepare_data.py \
    --input data/eeg_files/ \
    --output data/processed/ \
    --window-size 4.0 \
    --overlap 0.5
```

### **3. Model Training**

```bash
# Train the neural network
python scripts/train.py \
    --data data/processed/ \
    --epochs 100 \
    --batch-size 32 \
    --learning-rate 0.001
```

### **4. Evaluation**

```bash
# Test model performance
python scripts/evaluate.py \
    --model data/models/best_model.pth \
    --test-data data/processed/test/
```

## ğŸ® **Usage Modes**

### **Real-time Control**

```bash
# Live EEG streaming and robot control
python src/main.py --mode realtime --headset emotiv --robot ur_sim
```

### **File Processing**

```bash
# Process recorded EEG files
python src/main.py --mode inference --input data/eeg_files/session1.edf
```

### **Training Mode**

```bash
# Train new models
python src/main.py --mode train --data data/processed/ --epochs 50
```

### **Demo Mode**

```bash
# Demonstration without hardware
python src/demo.py --mode simulate --duration 60
```

## âš™ï¸ **Configuration**

### **Main Configuration** (`config/pipeline.yaml`)

```yaml
# EEG Processing
eeg:
  sample_rate: 256          # Sampling frequency (Hz)
  window_size: 4.0          # Analysis window (seconds)
  overlap: 0.5              # Window overlap ratio
  channels: 32              # Number of EEG channels
  
# Frequency Bands
frequency_bands:
  delta: [0.5, 4]           # Deep sleep, unconscious
  theta: [4, 8]             # Drowsiness, meditation
  alpha: [8, 12]            # Relaxed awareness
  beta: [12, 30]            # Active concentration
  gamma: [30, 45]           # High-level cognitive processing

# Model Parameters
model:
  architecture: "cnn_gcn_transformer"
  n_classes: 5              # Number of robot commands
  sequence_length: 10       # Temporal sequence length
  
# Robot Control
robot:
  command_rate: 10          # Commands per second
  movement_scale: 0.05      # Movement magnitude
  axes: ["x", "y", "z"]     # Controlled axes
```

### **Emotiv Configuration** (`config/emotiv.yaml`)

```yaml
# Emotiv Headset Settings
emotiv:
  client_id: "your_client_id"
  client_secret: "your_client_secret"
  
  # Headset preferences
  headset:
    model: "EPOC X"         # EPOC X, FLEX, Insight
    connection: "bluetooth"  # bluetooth, usb
    
  # Streaming settings
  stream:
    eeg: true               # Enable EEG streaming
    motion: false           # Disable motion data
    contact_quality: true   # Monitor electrode contact
    
  # Data quality
  quality:
    min_contact_quality: 2  # Minimum electrode quality (1-4)
    max_noise_level: 50     # Maximum noise threshold
```

## ğŸ¤– **Robot Commands**

The system generates movement commands in JSON format:

```json
{
  "dx": 0.05,    // X-axis movement (meters)
  "dy": 0.0,     // Y-axis movement (meters) 
  "dz": 0.0,     // Z-axis movement (meters)
  "drx": 0.0,    // X-axis rotation (radians)
  "dry": 0.0,    // Y-axis rotation (radians)
  "drz": 0.0     // Z-axis rotation (radians)
}
```

### **Command Mapping**

| Mental State | Command | Robot Action |
|-------------|---------|--------------|
| Rest/Baseline | Class 0 | No movement |
| Left Hand Imagery | Class 1 | Move left (-X) |
| Right Hand Imagery | Class 2 | Move right (+X) |
| Forward Imagery | Class 3 | Move forward (+Y) |
| Backward Imagery | Class 4 | Move backward (-Y) |

## ğŸ”¬ **Model Architecture**

### **Neural Network Components**

1. **3D CNN Layers**
   - Spatial-temporal feature extraction
   - Depth-wise convolutions for efficiency
   - Batch normalization and dropout

2. **Graph Convolutional Network (GCN)**
   - Models electrode connectivity
   - Captures spatial brain relationships
   - Learnable adjacency matrix

3. **Transformer Encoder**
   - Temporal sequence modeling
   - Multi-head self-attention
   - Positional encoding for time awareness

4. **Classification Head**
   - Fully connected layers
   - 5-class output for robot commands
   - Softmax activation for probabilities

### **Input/Output Specifications**

- **Input**: `(batch_size, n_electrodes, n_bands, time_steps)`
- **Example**: `(32, 32, 5, 10)` for 32 samples, 32 electrodes, 5 frequency bands, 10 time steps
- **Output**: `(batch_size, n_classes)` probability distribution over commands

## ğŸ“Š **Performance Metrics**

### **Real-time Performance**
- **Latency**: <100ms from EEG to robot command
- **Throughput**: 10 commands/second
- **Accuracy**: >85% classification accuracy (trained users)

### **Training Results**
- **Training Time**: ~2 hours on GPU for 100 epochs
- **Model Size**: ~15MB compressed
- **Memory Usage**: ~500MB during inference

## ğŸ”§ **Troubleshooting**

### **Common Issues**

**1. Emotiv Connection Problems**
```bash
# Check device connection
python src/eeg/emotiv_streamer.py --test-connection

# Verify credentials
python scripts/test_emotiv.py --check-auth
```

**2. Model Loading Errors**
```bash
# Download pre-trained weights
python scripts/download_models.py

# Verify model compatibility
python scripts/test_model.py --model data/models/best_model.pth
```

**3. Robot Communication Issues**
```bash
# Test robot connection
python src/robot/controller.py --test

# Check command file output
tail -f data/robot_commands.jsonl
```

### **Performance Optimization**

**For Better Accuracy:**
- Ensure good electrode contact (contact quality > 2)
- Minimize electrical interference
- Train with consistent mental imagery
- Use subject-specific model training

**For Lower Latency:**
- Use GPU acceleration
- Optimize buffer sizes
- Reduce model complexity if needed
- Enable real-time priority

## ğŸ§ª **Development**

### **Setting Up Development Environment**

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Install pre-commit hooks
pre-commit install

# Run tests
python -m pytest tests/
```

### **Code Quality**

```bash
# Format code
black src/ scripts/
isort src/ scripts/

# Lint code
flake8 src/ scripts/
pylint src/ scripts/

# Type checking
mypy src/
```

### **Contributing**

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/new-feature`
3. Make changes and add tests
4. Run quality checks: `./scripts/check_quality.sh`
5. Commit changes: `git commit -m "Add new feature"`
6. Push to branch: `git push origin feature/new-feature`
7. Create a Pull Request

## ğŸ“š **Documentation**

- **[API Documentation](docs/API.md)**: Detailed API reference
- **[Training Guide](docs/TRAINING.md)**: Comprehensive training tutorial
- **[Troubleshooting](docs/TROUBLESHOOTING.md)**: Common issues and solutions
- **[Examples](notebooks/)**: Jupyter notebook tutorials

## ğŸ“„ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ **Acknowledgments**

- **Emotiv Inc.** for EEG hardware and SDK
- **Universal Robots** for robot simulation tools
- **MNE Python** for signal processing capabilities
- **PyTorch** for machine learning framework

## ğŸ“ **Support**

- **Issues**: [GitHub Issues](https://github.com/MonashDeepNeuron/Motorola-Dream-Machine/issues)
- **Discussions**: [GitHub Discussions](https://github.com/MonashDeepNeuron/Motorola-Dream-Machine/discussions)
- **Email**: [contact@monashdeepneuron.org](mailto:contact@monashdeepneuron.org)

## ğŸš€ **Citation**

If you use this work in your research, please cite:

```bibtex
@software{eeg_robot_control_2025,
  title={EEG-to-Robot Control System},
  author={Monash DeepNeuron},
  year={2025},
  url={https://github.com/MonashDeepNeuron/Motorola-Dream-Machine}
}
```

---

**âš¡ Ready to control robots with your thoughts? Let's get started!** ğŸ§ ğŸ¤–
